# My Lab Nootbook for Ecological Genomics  

## Author: Csenge Petak    

### Start Date: 2020-01-13
### End Date: 2020-05-08
### Project Descriptions:   





# Table of Contents:   
* [Entry 1: 2020-01-13, Monday](#id-section1)
* [Entry 2: 2020-01-14, Tuesday](#id-section2)
* [Entry 3: 2020-01-15, Wednesday](#id-section3)
* [Entry 4: 2020-01-16, Thursday](#id-section4)
* [Entry 5: 2020-01-17, Friday](#id-section5)
* [Entry 6: 2020-01-20, Monday](#id-section6)
* [Entry 7: 2020-01-21, Tuesday](#id-section7)
* [Entry 8: 2020-01-22, Wednesday](#id-section8)
* [Entry 9: 2020-01-23, Thursday](#id-section9)
* [Entry 10: 2020-01-24, Friday](#id-section10)
* [Entry 11: 2020-01-27, Monday](#id-section11)
* [Entry 12: 2020-01-28, Tuesday](#id-section12)
* [Entry 13: 2020-01-29, Wednesday](#id-section13)
* [Entry 14: 2020-01-30, Thursday](#id-section14)
* [Entry 15: 2020-01-31, Friday](#id-section15)
* [Entry 16: 2020-02-03, Monday](#id-section16)
* [Entry 17: 2020-02-04, Tuesday](#id-section17)
* [Entry 18: 2020-02-05, Wednesday](#id-section18)
* [Entry 19: 2020-02-06, Thursday](#id-section19)
* [Entry 20: 2020-02-07, Friday](#id-section20)
* [Entry 21: 2020-02-10, Monday](#id-section21)
* [Entry 22: 2020-02-11, Tuesday](#id-section22)
* [Entry 23: 2020-02-12, Wednesday](#id-section23)
* [Entry 24: 2020-02-13, Thursday](#id-section24)
* [Entry 25: 2020-02-14, Friday](#id-section25)
* [Entry 26: 2020-02-17, Monday](#id-section26)
* [Entry 27: 2020-02-18, Tuesday](#id-section27)
* [Entry 28: 2020-02-19, Wednesday](#id-section28)
* [Entry 29: 2020-02-20, Thursday](#id-section29)
* [Entry 30: 2020-02-21, Friday](#id-section30)
* [Entry 31: 2020-02-24, Monday](#id-section31)
* [Entry 32: 2020-02-25, Tuesday](#id-section32)
* [Entry 33: 2020-02-26, Wednesday](#id-section33)
* [Entry 34: 2020-02-27, Thursday](#id-section34)
* [Entry 35: 2020-02-28, Friday](#id-section35)
* [Entry 36: 2020-03-02, Monday](#id-section36)
* [Entry 37: 2020-03-03, Tuesday](#id-section37)
* [Entry 38: 2020-03-04, Wednesday](#id-section38)
* [Entry 39: 2020-03-05, Thursday](#id-section39)
* [Entry 40: 2020-03-06, Friday](#id-section40)
* [Entry 41: 2020-03-09, Monday](#id-section41)
* [Entry 42: 2020-03-10, Tuesday](#id-section42)
* [Entry 43: 2020-03-11, Wednesday](#id-section43)
* [Entry 44: 2020-03-12, Thursday](#id-section44)
* [Entry 45: 2020-03-13, Friday](#id-section45)
* [Entry 46: 2020-03-16, Monday](#id-section46)
* [Entry 47: 2020-03-17, Tuesday](#id-section47)
* [Entry 48: 2020-03-18, Wednesday](#id-section48)
* [Entry 49: 2020-03-19, Thursday](#id-section49)
* [Entry 50: 2020-03-20, Friday](#id-section50)
* [Entry 51: 2020-03-23, Monday](#id-section51)
* [Entry 52: 2020-03-24, Tuesday](#id-section52)
* [Entry 53: 2020-03-25, Wednesday](#id-section53)
* [Entry 54: 2020-03-26, Thursday](#id-section54)
* [Entry 55: 2020-03-27, Friday](#id-section55)
* [Entry 56: 2020-03-30, Monday](#id-section56)
* [Entry 57: 2020-03-31, Tuesday](#id-section57)
* [Entry 58: 2020-04-01, Wednesday](#id-section58)
* [Entry 59: 2020-04-02, Thursday](#id-section59)
* [Entry 60: 2020-04-03, Friday](#id-section60)
* [Entry 61: 2020-04-06, Monday](#id-section61)
* [Entry 62: 2020-04-07, Tuesday](#id-section62)
* [Entry 63: 2020-04-08, Wednesday](#id-section63)
* [Entry 64: 2020-04-09, Thursday](#id-section64)
* [Entry 65: 2020-04-10, Friday](#id-section65)
* [Entry 66: 2020-04-13, Monday](#id-section66)
* [Entry 67: 2020-04-14, Tuesday](#id-section67)
* [Entry 68: 2020-04-15, Wednesday](#id-section68)
* [Entry 69: 2020-04-16, Thursday](#id-section69)
* [Entry 70: 2020-04-17, Friday](#id-section70)
* [Entry 71: 2020-04-20, Monday](#id-section71)
* [Entry 72: 2020-04-21, Tuesday](#id-section72)
* [Entry 73: 2020-04-22, Wednesday](#id-section73)
* [Entry 74: 2020-04-23, Thursday](#id-section74)
* [Entry 75: 2020-04-24, Friday](#id-section75)
* [Entry 76: 2020-04-27, Monday](#id-section76)
* [Entry 77: 2020-04-28, Tuesday](#id-section77)
* [Entry 78: 2020-04-29, Wednesday](#id-section78)
* [Entry 79: 2020-04-30, Thursday](#id-section79)
* [Entry 80: 2020-05-01, Friday](#id-section80)
* [Entry 81: 2020-05-04, Monday](#id-section81)
* [Entry 82: 2020-05-05, Tuesday](#id-section82)
* [Entry 83: 2020-05-06, Wednesday](#id-section83)
* [Entry 84: 2020-05-07, Thursday](#id-section84)
* [Entry 85: 2020-05-08, Friday](#id-section85)


------    
<div id='id-section1'/>   


### Entry 1: 2020-01-13, Monday.   



------    
<div id='id-section2'/>   


### Entry 2: 2020-01-14, Tuesday.   



------    
<div id='id-section3'/>   


### Entry 3: 2020-01-15, Wednesday.   



------    
<div id='id-section4'/>   


### Entry 4: 2020-01-16, Thursday.   



------    
<div id='id-section5'/>   


### Entry 5: 2020-01-17, Friday.   



------    
<div id='id-section6'/>   


### Entry 6: 2020-01-20, Monday.   



------    
<div id='id-section7'/>   


### Entry 7: 2020-01-21, Tuesday.   



------    
<div id='id-section8'/>   


### Entry 8: 2020-01-22, Wednesday.   

# Intro to connecting to unix servers and navigating the bash command-line \
https://pespenilab.github.io/Ecological-Genomics/Tutorial/2020-01-22_Command_Line_Unix.html

Practice basic commands and setting up folders. \
Import metadata from /data/project_data/RS_ExomeSeq.

------    
<div id='id-section9'/>   


### Entry 9: 2020-01-23, Thursday.   



------    
<div id='id-section10'/>   


### Entry 10: 2020-01-24, Friday.   



------    
<div id='id-section11'/>   


### Entry 11: 2020-01-27, Monday.   



------    
<div id='id-section12'/>   


### Entry 12: 2020-01-28, Tuesday.   



------    
<div id='id-section13'/>   


### Entry 13: 2020-01-29, Wednesday.   

# Population Genomics Day 1 \
Pipeline on: https://pespenilab.github.io/Ecological-Genomics/2020-01-29_PopGenomics_Day1.html

cd /data/project_data/RS_ExomeSeq/fastq/edge_fastq/
R1, R2 - 1 forward, 1 reverse read for each fragment (see handwritten notes)

zcat AB_05_R1_fastq.gz | head -n 4, need this because it is gz zipped ->

### Parts of the fastq file:

First line: starts with @, which machine sequenced it, when, which spot on slide, other metadata, barcodes at the end of the line

Second line: sequence (around 150pb) , N=any base

"+" =separator

Forth line: quality of each nucleotide, asqi code, confidence score that it was read correctly, more on this on website

### Vim to edit script for fastqc operation:
write bash script for quality check in vim:\
hit i for insert mode -> file on server in my account\
hit escape  - no longer in edit mode\
shift column - command, wq = write and quite, followed by the name of the file\

not executable yet, need permission -> chmod

bash fastqc.sh -> stuff in myresults -> git add --all . -> git commit -m "message" -> git push

look at htmls of fastqc

PCR duplicates - just result of amplification, shouldn't interpret it as the actual DNA sample!

### My script to do fastqc:
``````
#!/bin/bash

cd ~/Ecological_genomics/myresults/

mkdir fastqc

for file in /data/project_data/RS_ExomeSeq/fastq/edge_fastq/XCV*fastq.gz

do

# fastqc is a program we are calling that is already available, followed by input and output location
fastqc ${file} -o fastqc/ # because I'm going to be in myresults already

done
``````

### Trimming reads:
now we'll actually make changes to the data files, not just check quality\
output to whole class\
script written for using Trimmomatic - uses both reads at the same time\

### My script for trimming reads:
``````
#!/bin/bash   
 
cd /data/project_data/RS_ExomeSeq/fastq/edge_fastq  
 
for R1 in XCV*R1_fastq.gz  

do 
 
	R2=${R1/_R1_fastq.gz/_R2_fastq.gz}
	f=${R1/_R1_fastq.gz/}
	name=`basename ${f}`

	java -classpath /data/popgen/Trimmomatic-0.33/trimmomatic-0.33.jar org.usadellab.trimmomatic.TrimmomaticPE \
        -threads 1 \
        -phred33 \
         "$R1" \
         "$R2" \
         /data/project_data/RS_ExomeSeq/fastq/edge_fastq/pairedcleanreads/${name}_R1.cl.pd.fq \
         /data/project_data/RS_ExomeSeq/fastq/edge_fastq/unpairedcleanreads/${name}_R1.cl.un.fq \
         /data/project_data/RS_ExomeSeq/fastq/edge_fastq/pairedcleanreads/${name}_R2.cl.pd.fq \
         /data/project_data/RS_ExomeSeq/fastq/edge_fastq/unpairedcleanreads/${name}_R2.cl.un.fq \
        ILLUMINACLIP:/data/popgen/Trimmomatic-0.33/adapters/TruSeq3-PE.fa:2:30:10 \
        LEADING:20 \
        TRAILING:20 \
        SLIDINGWINDOW:6:20 \
        MINLEN:35 
 
done 
``````

Meaning of settings above:

+ Cut adapter and other illumina-specific sequences from the read
+ Cut bases off the start of a read, if below a threshold quality = 20
+ Cut bases off the end of a read, if below a threshold quality = 20
+ Scan the read with a 4-base wide sliding window, cutting when the average quality per base drops below 15 (SLIDINGWINDOW:4:15) = here 6:20
+ Drop the read if it is below a specified length = 35

------    
<div id='id-section14'/>   


### Entry 14: 2020-01-30, Thursday.   



------    
<div id='id-section15'/>   


### Entry 15: 2020-01-31, Friday.   



------    
<div id='id-section16'/>   


### Entry 16: 2020-02-03, Monday.   



------    
<div id='id-section17'/>   


### Entry 17: 2020-02-04, Tuesday.   



------    
<div id='id-section18'/>   


### Entry 18: 2020-02-05, Wednesday.   

# Population Genomics Day 2 \
Pipeline on: https://pespenilab.github.io/Ecological-Genomics/2020-02-05_PopGenomics_Day2.html

Last time we trimmed data -> now higher quality

Next up: **Mapping to reference** with BWA -> sam (can be looked at) -> bam (binary but better for storage)
-> remove PCR duplicates -> indexing -> mapping statistics
1st individual scripts to test -> then put it in a wrapper that calls all of them

There isn't a reference genome for this species so we are using a close relative: 20 Gb! too big -> reduced sequence based on our probes

there are only contigs, we don't know their relative location on chromosomes... like for most non-model organisms
**N50** - show state of the assembly, sort contigs based on size -> all together 668 Mb, take 50% -> length of contig at the 50% point = N50

call variable: ${myrepo}

When aligning you let some mismatches/indels

BWA = for this data a bunch of alignment algorithms were tried and this one had the best trande off between speed and accuracy
BWA-MEM = best for short sequences

-M -> allow reads to map on different contigs - those contigs could be close by on the chromosome

markdup -> removes PCR duplicates, so excess matches that map exactly the same genomic region
indexing is for quick look up and retreaval

### My script for "shell"
``````
#!/bin/bash

#Path to my repo:
myrepo="/users/c/p/cpetak/Ecological_genomics"

#My population:
mypop="XCV"

#Directory to our cleaned and paired reads:
input="/data/project_data/RS_ExomeSeq/fastq/edge_fastq/pairedcleanreads/${mypop}"

#Directory to store the outputs of our mapping:
output="/data/project_data/RS_ExomeSeq/mapping"

#Run mapping.sh
source ./mapping.sh

#Run the post-processing steps
source ./process_bam.sh
``````
### My script for BWA
``````
#!/bin/bash

#Path to reference genomes
ref="/data/project_data/RS_ExomeSeq/ReferenceGenomes/Pabies1.0-genome_reduced.fa"

#Write a loop to map each individual within my population
for forward in ${input}*_R1.cl.pd.fq

do 
	reverse=${forward/_R1.cl.pd.fq/_R2.cl.pd.fq}
	f=${forward/_R1.cl.pd.fq/}
	name=`basename ${f}`
	bwa mem -t 1 -M ${ref} ${forward} ${reverse} > ${output}/BWA/${name}.sam
done
``````

### My script for SAM to BAM (sambamba), remove PCR dup (markup), index 
``````
#!/bin/bash

#This is where our output sam files are going to get converted into binary format (bam)
#Then, we're going to sort the bam files, remove the PCR duplicates, index them

#First, let's convert sam to bam
for f in ${output}/BWA/${mypop}*.sam

do
	out=${f/.sam/}
	sambamba-0.7.1-linux-static view -S --format=bam ${f} -o ${out}.bam
	samtools sort ${out}.bam -o ${out}.sorted.bam
done

#Now let's remove the PCR duplicates
for file in ${out}/BWA/${mypop}*.sorted.bam

do
	f=${file/.sorted.bam/}
	sambamba-0.7.1-linux-static markdup -r -t 1 ${file} ${f}.sorted.rmdup.bam
done

#Now to finish we'll index our files

for file in ${output}/BWA/${mypop}*.sorted.rmdup.bam

do
	samtools index ${file}
done
``````

------    
<div id='id-section19'/>   


### Entry 19: 2020-02-06, Thursday.   



------    
<div id='id-section20'/>   


### Entry 20: 2020-02-07, Friday.   



------    
<div id='id-section21'/>   


### Entry 21: 2020-02-10, Monday.   



------    
<div id='id-section22'/>   


### Entry 22: 2020-02-11, Tuesday.   



------    
<div id='id-section23'/>   


### Entry 23: 2020-02-12, Wednesday.  
# Population Genomics Day 3 \
https://pespenilab.github.io/Ecological-Genomics/2020-02-12_PopGenomics_Day3.html

**Mapping statistics**
Output of BWA -> SAM file, includes: 
read, 
information about mapping success (orientation, left or right read), 
reference sequence name, 
mapping quality (Phred-scaled), 
CIGAR - alignment information Match (M), Insertion (I) or Deletion (D)),

-> BAM is in binary code, but same info

Summary of how well our reads mapped to the reference: 
samtools, 
-flagstat: basic info, 
-depth: depth of coverage, or how many reads cover each mapped position, on average

tview => look at a sam file, visualise (samtools tview filename)

### My script for mapping stats
``````
#!/bin/bash
myrepo="/users/c/p/cpetak/Ecological_genomics"
mypop="XCV"
output="/data/project_data/RS_ExomeSeq/mapping"

echo "Num.reads R1 R2 Paired MateMapped Singletons MateMappedDiffChr" > ${myrepo}/myresults/${mypop}.flagstats.txt

for file in ${output}/BWA/${mypop}*sorted.rmdup.bam
do
f=${file/.sorted.rmdup.bam/}
name=`basename ${f}`
echo ${name} >> ${myrepo}/myresults/${mypop}.names.txt
samtools flagstat ${file} | awk 'NR>=6&&NR<=13 {print $1}' | column -x >> ${myrepo}/myresults/${mypop}.flagstats.txt
done

for file in ${output}/BWA/${mypop}*sorted.rmdup.bam
do
samtools depth ${file} | awk '{sum+=$3} END {print sum/NR}' >> ${myrepo}/myresults/${mypop}.coverage.txt
done
``````

-> output of flagstat:
+ Num.reads is the total number of reads. Technically, this is the total number that bwa was able to map in any configuration and should correspond closely to the sum of cleaned paired reads for R1 and R2 for your individual post-Trimmomatic.
+ R1 and R2 are the total number of forward and reverse reads that mapped in any configuration.  These should sum to the 1st column.
+ Paired is the column perhaps of most interest.  It is the total number of reads (summed R1 and R2) that mapped “properly” — that is, where both reads are on the same chromosome, they are facing each other, and they are a distance apart that corresponds to their estimated insert length (insert lengths ~300-500 bp).
+ The “mate mapped”  is similar to the last category, but counts reads where both R1 and R2 mapped but not necessarily to same chromosome, orientation facing each other, or appropriate insert length. 
+ Singletons is how may reads mapped without the mate mapping somewhere
+ And mate mapped to a different chr means R1 and R2 both mapped, but are located on separate contigs.
+ For us, the most important info is in column 1 and 4 (Num. Reads and Paired).  The proportion of paired/num.reads tells us what proportion of our reads were mapped as expected.  Based on our exome capture results, you should expect something ~60-65%

**Discovering SNPs**
SNP calling step -> SNP 1 | Ind1 CC | Ind2 CT
but is ind1 really homozygote or there is just not enough coverage? -> using genotype likelihoods

Method: Analysis of Next Generation Sequence Data (ANGSD) 
use genotype likelihood to:
a. estimate the site frequency spectrum (SFS)
b. estimate nucleotide diversities
c. estimate Fst between all populations, or pairwise between sets of populations
d. perform a genetic PCA based on estimation of the genetic covariance matrix (this is done on the entire set of Edge ind’s)


------    
<div id='id-section24'/>   


### Entry 24: 2020-02-13, Thursday.   



------    
<div id='id-section25'/>   


### Entry 25: 2020-02-14, Friday.   



------    
<div id='id-section26'/>   


### Entry 26: 2020-02-17, Monday.   



------    
<div id='id-section27'/>   


### Entry 27: 2020-02-18, Tuesday.   



------    
<div id='id-section28'/>   


### Entry 28: 2020-02-19, Wednesday.   
# Population Genomics Day 4 \
https://pespenilab.github.io/Ecological-Genomics/2020-02-12_PopGenomics_Day4%20(1).html

**Site frequency spectrum (SFS)**
x-axis: # of individuals with derived allele (instead of anchestral), 
y-axis: # of sites
In real populations usually - lot of sites where every individual has the anchestral allele (so on x-axis 0 has high # of loci) and very few or no sites where every individuals has the derived allele (so on x-axis 2N (pop sizex2 because diploid) has low # of loci)
Example: y-axis: 200, 150, 100, 50, x-axis: 0,1,2,3 means that there are 200 sites where all individuals have the anchestral allele and there are 100 sites where 2 individuals have the derived allele.

Are we confident in the ancestral state of each variable site (SNP) in our dataset? ->
**Unfolded vs. folded SFS**

if we know the anestral state -> then the best info is contained in the unfolded spectrum
if we don't know -> make the SFS based on the minor allele (the less frequent allele; always < 0.5 in the population)

Folded spectra wraps the SFS around such that high frequency “derived” alleles are put in the small bins (low minor allele freq)

image in the tutorial: for loci where derived allele frequency more than 3 = more than 50% - they are regarded as the anchestral alleles in the folded SFS so at these loci the derived allele frequency changes from 4 to 2 (and 5 to 1 and 6 to 0) so they are going to be categorised as such -> 0,1,2 columns get taller

->> assuming the reference allele is also the ancestral allele can lead to biases in inference, and therefore the conservative thing to do is to “fold” the SFS and base it on the minor allele frequencies instead of the derived allele frequencies 
-> ‘fold 1’ flag when running the ANGSD program -> estimate the folded SFS for your pop with realSFS command

### My script for ANGSD
``````
#!/bin/bash

cd /users/c/p/cpetak/Ecological_genomics/myresults/ANGSD

mypop=XCV
REF="/data/project_data/RS_ExomeSeq/ReferenceGenomes/Pabies1.0-genome_reduced.fa"

ANGSD -b XCV_bam.list \
-ref ${REF} -anc ${REF} \
-out ${mypop}_outFold \
-nThreads 1 \
-remove_bads 1 \
-C 50 \
-baq 1 \
-minMapQ 20 \
-minQ 20 \
-setMinDepth 3 \
-minInd 2 \
-setMinDepthInd 1 \
-setMaxDepthInd 17 \
-skipTriallelic 1 \
-GL 1 \
-doCounts 1 \
-doMajorMinor 1 \
-doMaf 1 \
-doSaf 1 \
-pest ${mypop}_outFold.sfs \
-doThetas 1 \
-fold 1

thetaStat do_stat ${mypop}_outFold.thetas.idx
``````

**Computing nucleotide diversities and Tajima’s D from SFS**
ANGSD again, this time we include the -pest and the doThetas flag
-> ${mypop}.thetas.idx.pestPG -> into R

Per-site values of Watterson’s theta (tW) and theta-Pi (tP): divide each statistic by the total number of sites

### My R code
``````
SFS <- scan("XCV_outFold.sfs")
sumSFS <- sum(SFS)
pctPoly = 100*(1-(SFS[1]/sumSFS))
plotSFS <- SFS[-c(1,length(SFS)+1)]
barplot(plotSFS, xlab="XCV Pop SFS")
div <- read.table("XCV_outFold.thetas.idx.pestPG")
colnames(div)=c("window","chrome","wincenter","tW","tP","tF","tH","tL","tajD","fulif","fuliD","fayH","zengsE","numSites")
div$tWpersite = div$tW/div$numSites
div$tPpersite = div$tP/div$numSites
pdf("XWS_diversity_stats2.pdf")
par(mfrow=c(2,2))

hist(div$tWpersite,col="darkorchid",xlab="Theta_W",main="")
hist(div$tPpersite,col="darkorchid", xlab="Theta-Pi",main="")
hist(tajDEst,col="darkorchid",xlab="Tajima's D",main="")

summary(div)

barplot(plotSFS, main="SFS", xlab= "Derived allele frequency", ylab="Number of sites")
dev.off()
``````

------    
<div id='id-section29'/>   


### Entry 29: 2020-02-20, Thursday.   



------    
<div id='id-section30'/>   


### Entry 30: 2020-02-21, Friday.   



------    
<div id='id-section31'/>   


### Entry 31: 2020-02-24, Monday.   



------    
<div id='id-section32'/>   


### Entry 32: 2020-02-25, Tuesday.   



------    
<div id='id-section33'/>   


### Entry 33: 2020-02-26, Wednesday.   
# Transcriptomics Day 1 \
https://pespenilab.github.io/Ecological-Genomics/2020-02-26_RNA-seq_Day1.html

Red scrouce 3' RNA seq = 1 read!
Factors:        Level:
Treament        - control, hot, hot and dry
Source climate  - hotdry and coldwet
Time            - 0,5,10


Possible questions to answer with this data:
Do individuals coming from different climate sources have different gene expression in each condition? 
exp ~ Source climate + treatment + (Source climate * treatment)
in different time points?
exp ~ time + Source climate + (time * Source climate) + family
What GO terms are enriched?
What genes are important?
Connecting SNPs to expression differences! ask this from melissa, look it up!

useful site: Congenie.org

**Data Processing Pipeline:**

1. FastQC on raw reads –> Trimmomatic (done!) –> FastQC on cleaned reads
2. We have reference transcriptome from closely related species:
66,632 unigenes, consisting of 26,437 high-confidence gene models, 32,150 medium-confidence gene models, and 8,045 low-confidence gene models
3. Use Salmon to simulateously map reads and quantify abundance.
4. Import the data into DESeq2 in R for data normalization, visualization, and statistical tests for differential gene expression.

My data: ASC C and D

Fastqc before and after trimming.

------    
<div id='id-section34'/>   


### Entry 34: 2020-02-27, Thursday.   



------    
<div id='id-section35'/>   


### Entry 35: 2020-02-28, Friday.   



------    
<div id='id-section36'/>   


### Entry 36: 2020-03-02, Monday.   



------    
<div id='id-section37'/>   


### Entry 37: 2020-03-03, Tuesday.   



------    
<div id='id-section38'/>   


### Entry 38: 2020-03-04, Wednesday.   
# Transcriptomics Day 2 \
https://pespenilab.github.io/Ecological-Genomics/2020-03-044_RNA-seq_Day2.html

Index reference transcriptome. \
``````
salmon index -t Pabies1.0-all-cds.fna.gz -i Pabies_cds_index --decoys decoys.txt -k 31
``````

### My script to map to reference transcriptome using Salmon.\
``````
#!/bin/bash

output=/data/project_data/RS_RNASeq/salmon/cleanedreads

cd /data/project_data/RS_RNASeq/fastq/cleanreads/

for file in ASC_06_C*.cl.fq

do

salmon quant -i /data/project_data/RS_RNASeq/ReferenceTranscriptome/Pabies_cds_index -l A -r ${file} --validateMappings -o ${output}/${file}

done

for file2 in ASC_06_D*.cl.fq

do

salmon quant -i /data/project_data/RS_RNASeq/ReferenceTranscriptome/Pabies_cds_index -l A -r ${file2} --validateMappings -o ${output}/${file2}

done
``````

Check mapping rates:
``````
grep -r --include \*.log -e 'Mapping rate'
``````
Mine:
28.5599%, 41.9786%, 32.8468%, 22.5165%, 36.1068%
ASC_06_C_0, ASC_06_C_10, ASC_06_C_5, ASC_06_D_H_0, ASC_06_D_H_10
Percent of reads that mapped to reference

The output we want to look at: quant.sf within ASC_06_C_0_GAGTCC_R1.cl.fq
Combining individual quant.sf files into one data matrix with all 76 samples.
### R script
``````
library(tximportData)
library(tximport)

#locate the directory containing the files. 
dir <- "/data/project_data/RS_RNASeq/salmon/"
list.files(dir)

# read in table with sample ids
samples <- read.table("/data/project_data/RS_RNASeq/salmon/RS_samples.txt", header=TRUE)

# now point to quant files
all_files <- file.path(dir, samples$sample, "quant.sf")
names(all_files) <- samples$sample

# what would be used if linked transcripts to genes
#txi <- tximport(files, type = "salmon", tx2gene = tx2gene)
# to be able to run without tx2gene
txi <- tximport(all_files, type = "salmon", txOut=TRUE)  
names(txi)

head(txi$counts)

countsMatrix <- txi$counts
dim(countsMatrix)
#[1] 66069    76

# To write out
write.table(countsMatrix, file = "RS_countsMatrix.txt", col.names = T, row.names = T, quote = F) 
``````

------    
<div id='id-section39'/>   


### Entry 39: 2020-03-05, Thursday.   



------    
<div id='id-section40'/>   


### Entry 40: 2020-03-06, Friday.   



------    
<div id='id-section41'/>   


### Entry 41: 2020-03-09, Monday.   



------    
<div id='id-section42'/>   


### Entry 42: 2020-03-10, Tuesday.   



------    
<div id='id-section43'/>   


### Entry 43: 2020-03-11, Wednesday.   



------    
<div id='id-section44'/>   


### Entry 44: 2020-03-12, Thursday.   



------    
<div id='id-section45'/>   


### Entry 45: 2020-03-13, Friday.   



------    
<div id='id-section46'/>   


### Entry 46: 2020-03-16, Monday.   



------    
<div id='id-section47'/>   


### Entry 47: 2020-03-17, Tuesday.   



------    
<div id='id-section48'/>   


### Entry 48: 2020-03-18, Wednesday.  
# Transcriptomics Day 3 \
https://pespenilab.github.io/Ecological-Genomics/2020-03-17_RNA-seq_Day3.html

Last time we mapped to only exomes -> low quality. If we include 3'UTR as well -> our mapping rate improved dramatically, ranging from 40-70% of reads mapping across samples, mean of 52%. 

We now have the counts matrix and sample ID files.

Using DESeq in R!

## My R script
``````
## Set your working directory
setwd("~/Desktop")

## Import the libraries that we're likely to need in this session
library(DESeq2)
library(dplyr)
library(tidyr)
library(ggplot2)
library(scales)
library(ggpubr)
library(wesanderson)
library(vsn)  ### First: BiocManager::install("vsn") AND BiocManager::install("hexbin")

## Import the counts matrix
countsTable <- read.table("RS_cds2kb_countsMatrix.txt", header=TRUE, row.names=1)
head(countsTable)
dim(countsTable)
countsTableRound <- round(countsTable) # Need to round because DESeq wants only integers
head(countsTableRound)

## Import the samples description table - links each sample to factors of the experimental design.
# Need the colClasses otherwise imports "day" as numeric which DESeq doesn't like, coula altneratively change to d0, d5, d10
conds <- read.delim("RS_samples.txt", header=TRUE, stringsAsFactors = TRUE, row.names=1, colClasses=c('factor', 'factor', 'factor', 'factor'))
head(conds)
dim(conds)


## Let's see how many reads we have from each sample:
colSums(countsTableRound)
mean(colSums(countsTableRound))
barplot(colSums(countsTableRound), las=3, cex.names=0.5,names.arg = substring(colnames(countsTableRound),1,13))
abline(h=mean(colSums(countsTableRound)), col="blue", lwd =2)

## What is the average number of counts per gene?
rowSums(countsTableRound)
mean(rowSums(countsTableRound))
median(rowSums(countsTableRound)) # median is much lower -> not normally distributed gene experssion, some very low/high expression

## What is the average number of counts per gene per sample?

apply(countsTableRound,2,mean)

## Create a DESeq object and define the experimental design here with the tilde

dds <- DESeqDataSetFromMatrix(countData = countsTableRound, colData = conds, design = ~ climate + day + treatment)
dim(dds)

# Filter out genes with few reads, if we choose 76 as minimum that is on average 1 read per sample!
# this reduces the number of tests we have to from 66408 to 23887 -> have to correct less later because of the mulitple stat tests
dds <- dds[rowSums(counts(dds)) > 76]


## Run the DESeq model to test for differential gene expression: 1) estimate size factors (per sample), 2) estimate dispersion (per gene), 3) run negative binomial glm

dds <- DESeq(dds)

# List the results you've generated

resultsNames(dds) #factors it compared, based on line where we specified design (DESeqDataSetFromMatrix). here first factor was pop. since we had day0 first, it compared day5 to it and day10 to it. see below
#[1] "Intercept"            "pop_BRU_05_vs_ASC_06" "pop_CAM_02_vs_ASC_06"
#[4] "pop_ESC_01_vs_ASC_06" "pop_JAY_02_vs_ASC_06" "pop_KAN_04_vs_ASC_06"
#[7] "pop_LOL_02_vs_ASC_06" "pop_MMF_13_vs_ASC_06" "pop_NOR_02_vs_ASC_06"
#[10] "pop_XBM_07_vs_ASC_06" "day_10_vs_0"          "day_5_vs_0"          
#[13] "treatment_D_vs_C"     "treatment_H_vs_C"

#if we  have climate instead of pop in the design (DESeqDataSetFromMatrix):
resultsNames(dds)
#[1] "Intercept"        "climate_HD_vs_CW" "day_10_vs_0"      "day_5_vs_0"      
#[5] "treatment_D_vs_C" "treatment_H_vs_C"

-------------------------------------------------------------------------------

# Order and list and summarize results from specific contrasts (i.e. the 6 above)
# Here you set your adjusted p-value cutoff, can make summary tables of the number of genes differentially expressed (up- or down-regulated) for each contrast

res <- results(dds, alpha = 0.05) #automatically gives you last comparision
res <- res[order(res$padj),]
#-ve log change = more highly expressed in control! i.e. downregulated in treated

summary(res) #we get, "treatment H vs C"
#LFC > 0 (up)       : 16, 0.067%
#LFC < 0 (down)     : 3, 0.013% -> 3 were higher in control, down regulated in hot

#to pull out results from different contrasts
res_treatCD <- results(dds, name ="treatment_D_vs_C", alpha = 0.05)
res_treatCD <- res_treatCD[order(res_treatCD$padj),]
  
summary(res_treatCD) #we get, "treatment_D_vs_C"
#LFC > 0 (up)       : 678, 2.8% #up regulated in drought
#LFC < 0 (down)     : 424, 1.8% #up in control

-------------------------------------------------------------------------------

##### Data visualization #####
# MA plot
plotMA(res_treatCD, ylim=c(-3,3))
#below 0 = higher expression in control, red=significant

# PCA
vsd <- vst(dds, blind=FALSE)
#you can play with "nsub" operation of this function
data <- plotPCA(vsd, intgroup=c("climate", "treatment", "day"), returnData =TRUE) #can play with ntop, default 500 genes!
percentVar <- round(100*attr(data, "percentVar"))

data$treatment <- factor(data$treatment, levels=c("C","H","D"), labels = c("C","H","D"))
data$day <- factor(data$day, levels=c("0","5","10"), labels = c("0","5","10"))

ggplot(data, aes(PC1, PC2, color=day, shape=treatment)) +
  geom_point(size=4, alpha=0.85) +
  xlab(paste0("PC1: ",percentVar[1],"% variance")) +
  ylab(paste0("PC2: ",percentVar[2],"% variance")) +
  theme_minimal()

# Counts of specific top gene! (important validatition that the normalization, model is working)
#look at a few top genes, if they look similar to what you expect based on model significant result, result it is not driven by one spec gene
d <-plotCounts(dds, gene="MA_10257300g0010", intgroup = (c("treatment","climate","day")), returnData=TRUE)
d

p <-ggplot(d, aes(x=treatment, y=count, color=day, shape=climate)) + 
  theme_minimal() + theme(text = element_text(size=20), panel.grid.major=element_line(colour="grey"))
p <- p + geom_point(position=position_jitter(w=0.3,h=0), size=3) +
  scale_x_discrete(limits=c("C","H","D"))
p

# Heatmap of top 20 genes sorted by pvalue
library(pheatmap)
topgenes <- head(rownames(res_treatCD),20) #we ordered results by pvalue alread
mat <- assay(vsd)[topgenes,]
mat <- mat - rowMeans(mat)
df <- as.data.frame(colData(dds)[,c("treatment","climate", "day")])
pheatmap(mat, annotation_col=df)

--------------------------------------------------------------------

# Try with only Day 10 data, put this right after where we define conds and run everything again

# grep("10", names(countsTableRound), value = TRUE)
# day10countstable <- subset(countsTableRound, grep("10", names(countsTableRound), value = TRUE)) #doesn't work has to be logical

day10countstable <- countsTableRound %>% select(contains("10"))
dim(day10countstable)

conds10<- subset(conds, day=="10")
dim(conds10)
``````


------    
<div id='id-section49'/>   


### Entry 49: 2020-03-19, Thursday.   



------    
<div id='id-section50'/>   


### Entry 50: 2020-03-20, Friday.   



------    
<div id='id-section51'/>   


### Entry 51: 2020-03-23, Monday.   



------    
<div id='id-section52'/>   


### Entry 52: 2020-03-24, Tuesday.   



------    
<div id='id-section53'/>   


### Entry 53: 2020-03-25, Wednesday.   



------    
<div id='id-section54'/>   


### Entry 54: 2020-03-26, Thursday.   



------    
<div id='id-section55'/>   


### Entry 55: 2020-03-27, Friday.   



------    
<div id='id-section56'/>   


### Entry 56: 2020-03-30, Monday.   



------    
<div id='id-section57'/>   


### Entry 57: 2020-03-31, Tuesday.   



------    
<div id='id-section58'/>   


### Entry 58: 2020-04-01, Wednesday.   



------    
<div id='id-section59'/>   


### Entry 59: 2020-04-02, Thursday.   



------    
<div id='id-section60'/>   


### Entry 60: 2020-04-03, Friday.   



------    
<div id='id-section61'/>   


### Entry 61: 2020-04-06, Monday.   



------    
<div id='id-section62'/>   


### Entry 62: 2020-04-07, Tuesday.   



------    
<div id='id-section63'/>   


### Entry 63: 2020-04-08, Wednesday.   



------    
<div id='id-section64'/>   


### Entry 64: 2020-04-09, Thursday.   



------    
<div id='id-section65'/>   


### Entry 65: 2020-04-10, Friday.   



------    
<div id='id-section66'/>   


### Entry 66: 2020-04-13, Monday.   



------    
<div id='id-section67'/>   


### Entry 67: 2020-04-14, Tuesday.   



------    
<div id='id-section68'/>   


### Entry 68: 2020-04-15, Wednesday.   



------    
<div id='id-section69'/>   


### Entry 69: 2020-04-16, Thursday.   



------    
<div id='id-section70'/>   


### Entry 70: 2020-04-17, Friday.   



------    
<div id='id-section71'/>   


### Entry 71: 2020-04-20, Monday.   



------    
<div id='id-section72'/>   


### Entry 72: 2020-04-21, Tuesday.   



------    
<div id='id-section73'/>   


### Entry 73: 2020-04-22, Wednesday.   



------    
<div id='id-section74'/>   


### Entry 74: 2020-04-23, Thursday.   



------    
<div id='id-section75'/>   


### Entry 75: 2020-04-24, Friday.   



------    
<div id='id-section76'/>   


### Entry 76: 2020-04-27, Monday.   



------    
<div id='id-section77'/>   


### Entry 77: 2020-04-28, Tuesday.   



------    
<div id='id-section78'/>   


### Entry 78: 2020-04-29, Wednesday.   



------    
<div id='id-section79'/>   


### Entry 79: 2020-04-30, Thursday.   



------    
<div id='id-section80'/>   


### Entry 80: 2020-05-01, Friday.   



------    
<div id='id-section81'/>   


### Entry 81: 2020-05-04, Monday.   



------    
<div id='id-section82'/>   


### Entry 82: 2020-05-05, Tuesday.   



------    
<div id='id-section83'/>   


### Entry 83: 2020-05-06, Wednesday.   



------    
<div id='id-section84'/>   


### Entry 84: 2020-05-07, Thursday.   



------    
<div id='id-section85'/>   


### Entry 85: 2020-05-08, Friday.   



